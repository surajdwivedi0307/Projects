{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3MMAcssHTML"
      },
      "source": [
        "<link rel=\"stylesheet\" href=\"/site-assets/css/gemma.css\">\n",
        "<link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css2?family=Google+Symbols:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200\" />"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install Kaggle\n",
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "pc9sKFixlYI-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2024 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "_HOiWA5flkpv",
        "outputId": "e5692e11-c37e-4786-e4fb-3c565205456e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6b81c7de-d157-4921-aaec-8fffc8e4e747\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6b81c7de-d157-4921-aaec-8fffc8e4e747\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"surajpdwivedi\",\"key\":\"9f76e73ac9e28c8c1e75244dd17a6c3f\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a kaggle folder\n",
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "jZahu8kqmRIY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#copy the kaggle.json to folder created\n",
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "7oczalYRmgHw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#permission for the json to act\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "pW6QitTamxbC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to list all the dataset in kaggle\n",
        "! kaggle datasets list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46nvuhVUm8IM",
        "outputId": "b9f86761-af20-4c5e-9c8e-abb3f5cf0b1e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                          title                                           size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "-----------------------------------------------------------  ---------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "muhammadroshaanriaz/students-performance-dataset-cleaned     Students Performance | Clean Dataset            10KB  2024-10-29 19:32:26          10171        185  1.0              \n",
            "ikynahidwin/depression-student-dataset                       Depression Student Dataset                       4KB  2024-11-20 06:42:01           1449         32  1.0              \n",
            "steve1215rogg/student-lifestyle-dataset                      student lifestyle dataset                       22KB  2024-11-11 19:11:28           4851         70  1.0              \n",
            "valakhorasani/gym-members-exercise-dataset                   Gym Members Exercise Dataset                    22KB  2024-10-06 11:27:38          20553        290  1.0              \n",
            "ikynahidwin/depression-professional-dataset                  Depression Professional Dataset                 16KB  2024-11-20 06:39:18           1234         27  1.0              \n",
            "jacopoferretti/incomes-of-30k-usa-citizens                   Incomes of 30K USA Citizens                    468KB  2024-11-17 00:16:20           1199         25  1.0              \n",
            "jakewright/200k-youtube-channel-analytics                    200k YouTube Channel Analytics                   4MB  2024-11-13 16:31:28           1695         42  1.0              \n",
            "jahnavipaliwal/field-of-study-vs-occupation                  Field Of Study vs Occupation                   817KB  2024-11-07 20:38:44           1777         28  1.0              \n",
            "brsahan/vehicle-co2-emissions-dataset                        Vehicle CO2 Emissions Dataset                   88KB  2024-11-20 22:24:16            932         30  1.0              \n",
            "willianoliveiragibin/top-12-german-companies                 Top 12 German Companies                         23KB  2024-11-16 20:40:37            775         24  1.0              \n",
            "computingvictor/transactions-fraud-datasets                  💳 Financial Transactions Dataset: Analytics    348MB  2024-10-31 21:29:56           5052         77  1.0              \n",
            "octopusteam/full-imdb-dataset                                Full IMDb Dataset (1M+)                         20MB  2024-11-25 09:01:59           1047         22  1.0              \n",
            "ironwolf437/laptop-price-dataset                             Laptop Price - dataset‏                         25KB  2024-11-12 15:50:44           2487         45  1.0              \n",
            "taweilo/loan-approval-classification-data                    Loan Approval Classification Dataset           751KB  2024-10-29 04:07:34           5667         59  1.0              \n",
            "daniellopez01/credit-risk                                    credit_risk                                     13KB  2024-11-17 22:13:54           1185         32  1.0              \n",
            "ravindrasinghrana/carbon-co2-emissions                       Carbon (CO2) Emissions                          42KB  2024-10-28 09:11:56           2402         32  1.0              \n",
            "valakhorasani/bank-transaction-dataset-for-fraud-detection   Bank Transaction Dataset for Fraud Detection   102KB  2024-11-04 09:23:49           3143         59  1.0              \n",
            "valakhorasani/mobile-device-usage-and-user-behavior-dataset  Mobile Device Usage and User Behavior Dataset   11KB  2024-09-28 20:21:12          30473        566  1.0              \n",
            "mrmars1010/banana-quality-dataset                            Banana Quality dataset                          34KB  2024-11-07 08:52:35           1191         29  1.0              \n",
            "michau96/internet-search-engine-worldwide                    Internet search engine popularity                5KB  2024-11-10 17:45:09           1073         26  1.0              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c home-data-for-ml-course"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYxllntTn1Vo",
        "outputId": "d783a36e-6502-415e-ccda-c497f65f6299"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading home-data-for-ml-course.zip to /content\n",
            "100% 386k/386k [00:00<00:00, 1.02MB/s]\n",
            "100% 386k/386k [00:00<00:00, 1.02MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip home-data-for-ml-course.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BO4vOzEjoQ8U",
        "outputId": "0d0bac10-ed39-4b7a-e496-afc2a4e97b47"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  home-data-for-ml-course.zip\n",
            "  inflating: data_description.txt    \n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: sample_submission.csv.gz  \n",
            "  inflating: test.csv                \n",
            "  inflating: test.csv.gz             \n",
            "  inflating: train.csv               \n",
            "  inflating: train.csv.gz            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bGpuT-UFn0KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6aSSwSqymwmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qxv4Sn9b8CE"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://ai.google.dev/gemma/docs/gemma_chat\"><img src=\"https://ai.google.dev/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />View on ai.google.dev</a>\n",
        "  </td>\n",
        "    <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/gemma_chat.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/google/generative-ai-docs/main/site/en/gemma/docs/gemma_chat.ipynb\"><img src=\"https://ai.google.dev/images/cloud-icon.svg\" width=\"40\" />Open in Vertex AI</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/google/generative-ai-docs/blob/main/site/en/gemma/docs/gemma_chat.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "402c3d8a"
      },
      "source": [
        "# Building a chatbot with Gemma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b686fd95"
      },
      "source": [
        "Large Language Models (LLMs) such as Gemma excel at generating informative responses, making them ideal for building virtual assistants and chatbots.\n",
        "\n",
        "Conventionally, LLMs operate in a stateless manner, meaning they lack an inherent memory to store past conversations. Each prompt or question is processed independently, disregarding prior interactions. However, a crucial aspect of natural conversation is the ability to retain context from prior interactions. To overcome this limitation and enable LLMs to maintain conversation context, they must be explicitly provided with relevant information such as the conversation history (or pertinent parts) into each new prompt presented to the LLM.\n",
        "\n",
        "This tutorial shows you how to develop a chatbot using the instruction-tuned model variant of Gemma."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29732090"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ6W7NzRe1VM"
      },
      "source": [
        "### Gemma setup\n",
        "\n",
        "To complete this tutorial, you'll first need to complete the setup instructions at [Gemma setup](https://ai.google.dev/gemma/docs/setup). The Gemma setup instructions show you how to do the following:\n",
        "\n",
        "* Get access to Gemma on kaggle.com.\n",
        "* Select a Colab runtime with sufficient resources to run\n",
        "  the Gemma 2B model.\n",
        "* Generate and configure a Kaggle username and API key.\n",
        "\n",
        "After you've completed the Gemma setup, move on to the next section, where you'll set environment variables for your Colab environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gN-IVRC3dQe"
      },
      "source": [
        "### Set environment variables\n",
        "\n",
        "Set environment variables for `KAGGLE_USERNAME` and `KAGGLE_KEY`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now access the kaggle user name and key\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "user = UserSecretsClient()\n",
        "userdata = user.get_secret(\"kaggle_username\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "8B_YQClkpKak",
        "outputId": "c2423590-da47-4475-f3c7-cafdec1d5912"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'kaggle_secrets'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-41a858e5c12a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#now access the kaggle user name and key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkaggle_secrets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUserSecretsClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0muser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUserSecretsClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0muserdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_secret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kaggle_username\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kaggle_secrets'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Define the path to the uploaded kaggle.json file\n",
        "kaggle_json_path = '/content/kaggle.json'\n",
        "\n",
        "# Load the credentials from kaggle.json\n",
        "with open(kaggle_json_path, 'r') as f:\n",
        "    kaggle_creds = json.load(f)\n",
        "\n",
        "# Set Kaggle credentials as environment variables\n",
        "os.environ['KAGGLE_USERNAME'] = kaggle_creds['username']\n",
        "os.environ['KAGGLE_KEY'] = kaggle_creds['key']\n",
        "\n",
        "# Confirm the setup by listing datasets\n",
        "!kaggle datasets list\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQpkHQs3t8L7",
        "outputId": "6b0b0981-3b23-452f-e833-d71f6fde78b7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                          title                                           size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "-----------------------------------------------------------  ---------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "muhammadroshaanriaz/students-performance-dataset-cleaned     Students Performance | Clean Dataset            10KB  2024-10-29 19:32:26          10185        185  1.0              \n",
            "ikynahidwin/depression-student-dataset                       Depression Student Dataset                       4KB  2024-11-20 06:42:01           1465         32  1.0              \n",
            "steve1215rogg/student-lifestyle-dataset                      student lifestyle dataset                       22KB  2024-11-11 19:11:28           4860         70  1.0              \n",
            "valakhorasani/gym-members-exercise-dataset                   Gym Members Exercise Dataset                    22KB  2024-10-06 11:27:38          20567        290  1.0              \n",
            "ikynahidwin/depression-professional-dataset                  Depression Professional Dataset                 16KB  2024-11-20 06:39:18           1236         27  1.0              \n",
            "jacopoferretti/incomes-of-30k-usa-citizens                   Incomes of 30K USA Citizens                    468KB  2024-11-17 00:16:20           1202         25  1.0              \n",
            "jakewright/200k-youtube-channel-analytics                    200k YouTube Channel Analytics                   4MB  2024-11-13 16:31:28           1701         42  1.0              \n",
            "jahnavipaliwal/field-of-study-vs-occupation                  Field Of Study vs Occupation                   817KB  2024-11-07 20:38:44           1785         28  1.0              \n",
            "brsahan/vehicle-co2-emissions-dataset                        Vehicle CO2 Emissions Dataset                   88KB  2024-11-20 22:24:16            934         30  1.0              \n",
            "willianoliveiragibin/top-12-german-companies                 Top 12 German Companies                         23KB  2024-11-16 20:40:37            778         24  1.0              \n",
            "computingvictor/transactions-fraud-datasets                  💳 Financial Transactions Dataset: Analytics    348MB  2024-10-31 21:29:56           5054         77  1.0              \n",
            "octopusteam/full-imdb-dataset                                Full IMDb Dataset (1M+)                         20MB  2024-11-25 09:01:59           1048         22  1.0              \n",
            "ironwolf437/laptop-price-dataset                             Laptop Price - dataset‏                         25KB  2024-11-12 15:50:44           2490         45  1.0              \n",
            "taweilo/loan-approval-classification-data                    Loan Approval Classification Dataset           751KB  2024-10-29 04:07:34           5671         59  1.0              \n",
            "daniellopez01/credit-risk                                    credit_risk                                     13KB  2024-11-17 22:13:54           1187         32  1.0              \n",
            "ravindrasinghrana/carbon-co2-emissions                       Carbon (CO2) Emissions                          42KB  2024-10-28 09:11:56           2403         32  1.0              \n",
            "valakhorasani/bank-transaction-dataset-for-fraud-detection   Bank Transaction Dataset for Fraud Detection   102KB  2024-11-04 09:23:49           3152         59  1.0              \n",
            "valakhorasani/mobile-device-usage-and-user-behavior-dataset  Mobile Device Usage and User Behavior Dataset   11KB  2024-09-28 20:21:12          30477        567  1.0              \n",
            "mrmars1010/banana-quality-dataset                            Banana Quality dataset                          34KB  2024-11-07 08:52:35           1193         29  1.0              \n",
            "michau96/internet-search-engine-worldwide                    Internet search engine popularity                5KB  2024-11-10 17:45:09           1073         26  1.0              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9oy3QUmXtSd"
      },
      "source": [
        "### Install dependencies\n",
        "\n",
        "Install Keras and KerasNLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "a973dd7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13716ea6-5c08-4c5e-95fe-28b27aea2cb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.0/230.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires tensorboard<2.18,>=2.17, but you have tensorboard 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.1/644.1 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-cpu 2.18.0 requires tensorboard<2.19,>=2.18, but you have tensorboard 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-cpu 2.18.0 requires tensorboard<2.19,>=2.18, but you have tensorboard 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
        "!pip install -q tensorflow-cpu\n",
        "!pip install -q -U keras-nlp tensorflow-hub\n",
        "!pip install -q -U \"keras>=3\"\n",
        "!pip install -q -U tensorflow-text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wme8666dUPVR"
      },
      "source": [
        "### Select a backend\n",
        "\n",
        "Keras is a high-level, multi-framework deep learning API designed for simplicity and ease of use. [Keras 3](https://keras.io/keras_3){:.external} lets you choose the backend: TensorFlow, JAX, or PyTorch. All three will work for this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "67d12d2d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Select JAX as the backend\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
        "\n",
        "# Pre-allocate 100% of TPU memory to minimize memory fragmentation\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ajm_SGWTUjVd"
      },
      "source": [
        "### Import packages\n",
        "\n",
        "Import Keras and KerasNLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "3lyn9FxPUok8"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import keras_nlp\n",
        "\n",
        "# for reproducibility\n",
        "keras.utils.set_random_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39dc9d5b"
      },
      "source": [
        "### Instantiate the model\n",
        "\n",
        "KerasNLP provides implementations of many popular [model architectures](https://keras.io/api/keras_nlp/models/){:.external}. In this tutorial, you'll instantiate the model using `GemmaCausalLM`, an end-to-end Gemma model for causal language modeling. A causal language model predicts the next token based on previous tokens.\n",
        "\n",
        "Instantiate the model using the `from_preset` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "c86dc8fe"
      },
      "outputs": [],
      "source": [
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_instruct_2b_en\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcCv0BSdVFv9"
      },
      "source": [
        "The `GemmaCausalLM.from_preset()` function instantiates the model from a preset architecture and weights. In the code above, the string `\"gemma2_instruct_2b_en\"` specifies the preset the Gemma 2 2B model with 2 billion parameters. Gemma models with [7B, 9B, and 27B parameters](https://ai.google.com/gemma/docs/get_started#models-list) are also available. You can find the code strings for Gemma models in their **Model Variation** listings on [Kaggle](https://www.kaggle.com/models/google/gemma).\n",
        "\n",
        "Note: To run the larger models in Colab, you need access to the premium GPUs available in paid plans. Alternatively, you can perform inferences using Kaggle notebooks or Google Cloud projects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLNx8AoeVe-a"
      },
      "source": [
        "Use the `summary` method to get more info about the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "3MorieIpVksu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "ec1d490e-c4a4-439b-9f22-c81c4e739657"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n",
              "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
              "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,614,341,888\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "gemma_lm.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArZPOzFpVp6S"
      },
      "source": [
        "As you can see from the summary, the model has 2.6 billion trainable parameters.\n",
        "\n",
        "Note: For purposes of naming the model (\"2B\"), the embedding layer is not counted against the number of parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WpS39TBYql9"
      },
      "source": [
        "### Define formatting helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "3-obTC1jZGpZ"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown\n",
        "import textwrap\n",
        "\n",
        "def display_chat(prompt, text):\n",
        "  formatted_prompt = \"<font size='+1' color='brown'>🙋‍♂️<blockquote>\" + prompt + \"</blockquote></font>\"\n",
        "  text = text.replace('•', '  *')\n",
        "  text = textwrap.indent(text, '> ', predicate=lambda _: True)\n",
        "  formatted_text = \"<font size='+1' color='teal'>🤖\\n\\n\" + text + \"\\n</font>\"\n",
        "  return Markdown(formatted_prompt+formatted_text)\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ca54e8c"
      },
      "source": [
        "## Building the chatbot\n",
        "\n",
        "The Gemma instruction-tuned model `gemma2_instruct_2b_en` is fine-tuned to understand the following turn tokens:\n",
        "\n",
        "```\n",
        "<start_of_turn>user\\n  ... <end_of_turn>\\n\n",
        "<start_of_turn>model\\n ... <end_of_turn>\\n\n",
        "```\n",
        "\n",
        "This tutorial uses these tokens to build the chatbot. Refer to [Formatting and system instructions](https://ai.google.dev/gemma/docs/formatting) for more information on Gemma control tokens.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9583dfd1"
      },
      "source": [
        "### Create a chat helper to manage the conversation state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "e4e9a187"
      },
      "outputs": [],
      "source": [
        "class ChatState():\n",
        "  \"\"\"\n",
        "  Manages the conversation history for a turn-based chatbot\n",
        "  Follows the turn-based conversation guidelines for the Gemma family of models\n",
        "  documented at https://ai.google.dev/gemma/docs/formatting\n",
        "  \"\"\"\n",
        "\n",
        "  __START_TURN_USER__ = \"<start_of_turn>user\\n\"\n",
        "  __START_TURN_MODEL__ = \"<start_of_turn>model\\n\"\n",
        "  __END_TURN__ = \"<end_of_turn>\\n\"\n",
        "\n",
        "  def __init__(self, model, system=\"\"):\n",
        "    \"\"\"\n",
        "    Initializes the chat state.\n",
        "\n",
        "    Args:\n",
        "        model: The language model to use for generating responses.\n",
        "        system: (Optional) System instructions or bot description.\n",
        "    \"\"\"\n",
        "    self.model = model\n",
        "    self.system = system\n",
        "    self.history = []\n",
        "\n",
        "  def add_to_history_as_user(self, message):\n",
        "      \"\"\"\n",
        "      Adds a user message to the history with start/end turn markers.\n",
        "      \"\"\"\n",
        "      self.history.append(self.__START_TURN_USER__ + message + self.__END_TURN__)\n",
        "\n",
        "  def add_to_history_as_model(self, message):\n",
        "      \"\"\"\n",
        "      Adds a model response to the history with start/end turn markers.\n",
        "      \"\"\"\n",
        "      self.history.append(self.__START_TURN_MODEL__ + message)\n",
        "\n",
        "  def get_history(self):\n",
        "      \"\"\"\n",
        "      Returns the entire chat history as a single string.\n",
        "      \"\"\"\n",
        "      return \"\".join([*self.history])\n",
        "\n",
        "  def get_full_prompt(self):\n",
        "    \"\"\"\n",
        "    Builds the prompt for the language model, including history and system description.\n",
        "    \"\"\"\n",
        "    prompt = self.get_history() + self.__START_TURN_MODEL__\n",
        "    if len(self.system)>0:\n",
        "      prompt = self.system + \"\\n\" + prompt\n",
        "    return prompt\n",
        "\n",
        "  def send_message(self, message):\n",
        "    \"\"\"\n",
        "    Handles sending a user message and getting a model response.\n",
        "\n",
        "    Args:\n",
        "        message: The user's message.\n",
        "\n",
        "    Returns:\n",
        "        The model's response.\n",
        "    \"\"\"\n",
        "    self.add_to_history_as_user(message)\n",
        "    prompt = self.get_full_prompt()\n",
        "    response = self.model.generate(prompt, max_length=2048)\n",
        "    result = response.replace(prompt, \"\")  # Extract only the new response\n",
        "    self.add_to_history_as_model(result)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hmJS4h4ZmiP"
      },
      "source": [
        "### Chat with the model\n",
        "\n",
        "Start chatting with the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "b1913181",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "311cf89f-d50c-463e-a083-19cfb3c7ccbd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<font size='+1' color='brown'>🙋‍♂️<blockquote>Tell me, in a few words,  how to compute all prime numbers up to 1000?</blockquote></font><font size='+1' color='teal'>🤖\n\n> **Sieve of Eratosthenes.** \n> <end_of_turn>\n</font>"
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "chat = ChatState(gemma_lm)\n",
        "message = \"Tell me, in a few words,  how to compute all prime numbers up to 1000?\"\n",
        "display_chat(message, chat.send_message(message))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODKxUPP2Zuqy"
      },
      "source": [
        "Continue the conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "7448005b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "outputId": "a368d2c8-7326-4c09-842d-bdab9e54b106"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<font size='+1' color='brown'>🙋‍♂️<blockquote>Now in Python! No numpy, please!</blockquote></font><font size='+1' color='teal'>🤖\n\n> ```python\n> def sieve_of_eratosthenes(n):\n>   \"\"\"Returns a list of prime numbers up to n.\"\"\"\n>   primes = [True] * (n + 1)\n>   primes[0] = primes[1] = False\n>   for i in range(2, int(n**0.5) + 1):\n>     if primes[i]:\n>       for j in range(i * i, n + 1, i):\n>         primes[j] = False\n>   return [i for i, is_prime in enumerate(primes) if is_prime]\n> \n> primes = sieve_of_eratosthenes(1000)\n> print(primes)\n> ```\n> \n> **Explanation:**\n> \n> 1. **Initialization:**\n>    - `primes = [True] * (n + 1)`: Creates a list `primes` of boolean values, initially assuming all numbers are prime.\n>    - `primes[0] = primes[1] = False`: Sets 0 and 1 as non-prime.\n> \n> 2. **Iteration:**\n>    - `for i in range(2, int(n**0.5) + 1):`: Iterates from 2 to the square root of `n`. We only need to check up to the square root because any composite number must have a prime factor less than or equal to its square root.\n>    - `if primes[i]:`: If `i` is marked as prime:\n>      - `for j in range(i * i, n + 1, i):`: Marks all multiples of `i` as non-prime.\n> \n> 3. **Result:**\n>    - `return [i for i, is_prime in enumerate(primes) if is_prime]`: Creates a list of indices where `primes[i]` is True, representing the prime numbers.\n> \n> \n> Let me know if you'd like a more detailed explanation of any part! \n> <end_of_turn>\n</font>"
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "message = \"Now in Python! No numpy, please!\"\n",
        "display_chat(message, chat.send_message(message))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "0973ff54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825
        },
        "outputId": "e2c2cb91-5155-4207-bd93-36facacac6c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<font size='+1' color='brown'>🙋‍♂️<blockquote>Thank you, it works! Can you explain the code in French?</blockquote></font><font size='+1' color='teal'>🤖\n\n> Bien sûr ! Voici une explication du code en français :\n> \n> ```python\n> def sieve_of_eratosthenes(n):\n>   \"\"\"Retourne une liste de nombres premiers jusqu'à n.\"\"\"\n>   primes = [True] * (n + 1)\n>   primes[0] = primes[1] = False\n>   for i in range(2, int(n**0.5) + 1):\n>     if primes[i]:\n>       for j in range(i * i, n + 1, i):\n>         primes[j] = False\n>   return [i for i, is_prime in enumerate(primes) if is_prime]\n> \n> primes = sieve_of_eratosthenes(1000)\n> print(primes)\n> ```\n> \n> **Explication:**\n> \n> 1. **Initialisation:**\n>    - `primes = [True] * (n + 1)`: Crée une liste `primes` de valeurs booléennes, initialement supposant que tous les nombres sont premiers.\n>    - `primes[0] = primes[1] = False`: Définit 0 et 1 comme non-premiers.\n> \n> 2. **Itération:**\n>    - `for i in range(2, int(n**0.5) + 1):`: Itère de 2 jusqu'à la racine carrée de `n`. Nous ne devons vérifier que jusqu'à la racine carrée car tout nombre composite doit avoir un facteur premier inférieur ou égal à sa racine carrée.\n>    - `if primes[i]:`: Si `i` est considéré comme premier:\n>      - `for j in range(i * i, n + 1, i):`:  Marquer tous les multiples de `i` comme non-premiers.\n> \n> 3. **Resultat:**\n>    - `return [i for i, is_prime in enumerate(primes) if is_prime]`: Crée une liste des indices où `primes[i]` est vrai, représentant les nombres premiers.\n> \n> \n> N'hésitez pas à me demander si vous avez besoin d'une explication plus détaillée de quelque chose! \n> <end_of_turn>\n</font>"
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "message = \"Thank you, it works! Can you explain the code in French?\"\n",
        "display_chat(message, chat.send_message(message))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "a0c51f42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 883
        },
        "outputId": "9d785c4a-69e0-45ff-f4c6-7a768dfa481d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<font size='+1' color='brown'>🙋‍♂️<blockquote>Great! Now add those explanations as comments in the code.</blockquote></font><font size='+1' color='teal'>🤖\n\n> ```python\n> def sieve_of_eratosthenes(n):\n>   \"\"\"Retourne une liste de nombres premiers jusqu'à n.\"\"\"\n>   # Initialise une liste de boolean avec True pour tous les nombres de 0 à n\n>   primes = [True] * (n + 1)\n>   # Définit 0 et 1 comme non-premiers\n>   primes[0] = primes[1] = False\n>   # Itère de 2 à la racine carrée de n\n>   for i in range(2, int(n**0.5) + 1):\n>     # Si i est considéré comme premier\n>     if primes[i]:\n>       # Itère sur tous les multiples de i\n>       for j in range(i * i, n + 1, i):\n>         # Définit les multiples de i comme non-premiers\n>         primes[j] = False\n>   # Retourne la liste des indices des nombres premiers\n>   return [i for i, is_prime in enumerate(primes) if is_prime]\n> \n> primes = sieve_of_eratosthenes(1000)\n> print(primes)\n> ```\n> \n> **Explication:**\n> \n> * **Initialisation:**\n>     * `primes = [True] * (n + 1)`:  Crée une liste `primes` de valeurs booléennes, initialement supposant que tous les nombres sont premiers.\n>     * `primes[0] = primes[1] = False`: Définit 0 et 1 comme non-premiers.\n> * **Itération:**\n>     * `for i in range(2, int(n**0.5) + 1):`: Itère de 2 jusqu'à la racine carrée de `n`. Nous ne devons vérifier que jusqu'à la racine carrée car tout nombre composite doit avoir un facteur premier inférieur ou égal à sa racine carrée.\n>     * `if primes[i]:`: Si `i` est considéré comme premier:\n>         * `for j in range(i * i, n + 1, i):`:  Marquer tous les multiples de `i` comme non-premiers.\n> * **Resultat:**\n>     * `return [i for i, is_prime in enumerate(primes) if is_prime]`: Crée une liste des indices où `primes[i]` est vrai, représentant les nombres premiers. \n> \n> \n> \n> <end_of_turn>\n</font>"
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "message = \"Great! Now add those explanations as comments in the code.\"\n",
        "display_chat(message, chat.send_message(message))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51a33627"
      },
      "source": [
        "Test the generated response by running the generated code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "221c0817",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a98d85e-4404-42ca-e483-91d3eeb2b4a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229, 233, 239, 241, 251, 257, 263, 269, 271, 277, 281, 283, 293, 307, 311, 313, 317, 331, 337, 347, 349, 353, 359, 367, 373, 379, 383, 389, 397, 401, 409, 419, 421, 431, 433, 439, 443, 449, 457, 461, 463, 467, 479, 487, 491, 499, 503, 509, 521, 523, 541, 547, 557, 563, 569, 571, 577, 587, 593, 599, 601, 607, 613, 617, 619, 631, 641, 643, 647, 653, 659, 661, 673, 677, 683, 691, 701, 709, 719, 727, 733, 739, 743, 751, 757, 761, 769, 773, 787, 797, 809, 811, 821, 823, 827, 829, 839, 853, 857, 859, 863, 877, 881, 883, 887, 907, 911, 919, 929, 937, 941, 947, 953, 967, 971, 977, 983, 991, 997]\n"
          ]
        }
      ],
      "source": [
        "def sieve_of_eratosthenes(n):\n",
        "  \"\"\"Retourne une liste de nombres premiers jusqu'à n.\"\"\"\n",
        "  # Initialise une liste de boolean avec True pour tous les nombres de 0 à n\n",
        "  primes = [True] * (n + 1)\n",
        "  # Définit 0 et 1 comme non-premiers\n",
        "  primes[0] = primes[1] = False\n",
        "  # Itère de 2 à la racine carrée de n\n",
        "  for i in range(2, int(n**0.5) + 1):\n",
        "    # Si i est considéré comme premier\n",
        "    if primes[i]:\n",
        "      # Itère sur tous les multiples de i\n",
        "      for j in range(i * i, n + 1, i):\n",
        "        # Définit les multiples de i comme non-premiers\n",
        "        primes[j] = False\n",
        "  # Retourne la liste des indices des nombres premiers\n",
        "  return [i for i, is_prime in enumerate(primes) if is_prime]\n",
        "\n",
        "primes = sieve_of_eratosthenes(1000)\n",
        "print(primes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c8ece6c"
      },
      "source": [
        "Use the `get_history` method to see how all the context was retained by the `Chat` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "e48f4ca1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e59ff046-a61b-4019-90bc-2bfb1d96acf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start_of_turn>user\n",
            "Tell me, in a few words,  how to compute all prime numbers up to 1000?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "**Sieve of Eratosthenes.** \n",
            "<end_of_turn><start_of_turn>user\n",
            "Now in Python! No numpy, please!<end_of_turn>\n",
            "<start_of_turn>model\n",
            "```python\n",
            "def sieve_of_eratosthenes(n):\n",
            "  \"\"\"Returns a list of prime numbers up to n.\"\"\"\n",
            "  primes = [True] * (n + 1)\n",
            "  primes[0] = primes[1] = False\n",
            "  for i in range(2, int(n**0.5) + 1):\n",
            "    if primes[i]:\n",
            "      for j in range(i * i, n + 1, i):\n",
            "        primes[j] = False\n",
            "  return [i for i, is_prime in enumerate(primes) if is_prime]\n",
            "\n",
            "primes = sieve_of_eratosthenes(1000)\n",
            "print(primes)\n",
            "```\n",
            "\n",
            "**Explanation:**\n",
            "\n",
            "1. **Initialization:**\n",
            "   - `primes = [True] * (n + 1)`: Creates a list `primes` of boolean values, initially assuming all numbers are prime.\n",
            "   - `primes[0] = primes[1] = False`: Sets 0 and 1 as non-prime.\n",
            "\n",
            "2. **Iteration:**\n",
            "   - `for i in range(2, int(n**0.5) + 1):`: Iterates from 2 to the square root of `n`. We only need to check up to the square root because any composite number must have a prime factor less than or equal to its square root.\n",
            "   - `if primes[i]:`: If `i` is marked as prime:\n",
            "     - `for j in range(i * i, n + 1, i):`: Marks all multiples of `i` as non-prime.\n",
            "\n",
            "3. **Result:**\n",
            "   - `return [i for i, is_prime in enumerate(primes) if is_prime]`: Creates a list of indices where `primes[i]` is True, representing the prime numbers.\n",
            "\n",
            "\n",
            "Let me know if you'd like a more detailed explanation of any part! \n",
            "<end_of_turn><start_of_turn>user\n",
            "Thank you, it works! Can you explain the code in French?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Bien sûr ! Voici une explication du code en français :\n",
            "\n",
            "```python\n",
            "def sieve_of_eratosthenes(n):\n",
            "  \"\"\"Retourne une liste de nombres premiers jusqu'à n.\"\"\"\n",
            "  primes = [True] * (n + 1)\n",
            "  primes[0] = primes[1] = False\n",
            "  for i in range(2, int(n**0.5) + 1):\n",
            "    if primes[i]:\n",
            "      for j in range(i * i, n + 1, i):\n",
            "        primes[j] = False\n",
            "  return [i for i, is_prime in enumerate(primes) if is_prime]\n",
            "\n",
            "primes = sieve_of_eratosthenes(1000)\n",
            "print(primes)\n",
            "```\n",
            "\n",
            "**Explication:**\n",
            "\n",
            "1. **Initialisation:**\n",
            "   - `primes = [True] * (n + 1)`: Crée une liste `primes` de valeurs booléennes, initialement supposant que tous les nombres sont premiers.\n",
            "   - `primes[0] = primes[1] = False`: Définit 0 et 1 comme non-premiers.\n",
            "\n",
            "2. **Itération:**\n",
            "   - `for i in range(2, int(n**0.5) + 1):`: Itère de 2 jusqu'à la racine carrée de `n`. Nous ne devons vérifier que jusqu'à la racine carrée car tout nombre composite doit avoir un facteur premier inférieur ou égal à sa racine carrée.\n",
            "   - `if primes[i]:`: Si `i` est considéré comme premier:\n",
            "     - `for j in range(i * i, n + 1, i):`:  Marquer tous les multiples de `i` comme non-premiers.\n",
            "\n",
            "3. **Resultat:**\n",
            "   - `return [i for i, is_prime in enumerate(primes) if is_prime]`: Crée une liste des indices où `primes[i]` est vrai, représentant les nombres premiers.\n",
            "\n",
            "\n",
            "N'hésitez pas à me demander si vous avez besoin d'une explication plus détaillée de quelque chose! \n",
            "<end_of_turn><start_of_turn>user\n",
            "Great! Now add those explanations as comments in the code.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "```python\n",
            "def sieve_of_eratosthenes(n):\n",
            "  \"\"\"Retourne une liste de nombres premiers jusqu'à n.\"\"\"\n",
            "  # Initialise une liste de boolean avec True pour tous les nombres de 0 à n\n",
            "  primes = [True] * (n + 1)\n",
            "  # Définit 0 et 1 comme non-premiers\n",
            "  primes[0] = primes[1] = False\n",
            "  # Itère de 2 à la racine carrée de n\n",
            "  for i in range(2, int(n**0.5) + 1):\n",
            "    # Si i est considéré comme premier\n",
            "    if primes[i]:\n",
            "      # Itère sur tous les multiples de i\n",
            "      for j in range(i * i, n + 1, i):\n",
            "        # Définit les multiples de i comme non-premiers\n",
            "        primes[j] = False\n",
            "  # Retourne la liste des indices des nombres premiers\n",
            "  return [i for i, is_prime in enumerate(primes) if is_prime]\n",
            "\n",
            "primes = sieve_of_eratosthenes(1000)\n",
            "print(primes)\n",
            "```\n",
            "\n",
            "**Explication:**\n",
            "\n",
            "* **Initialisation:**\n",
            "    * `primes = [True] * (n + 1)`:  Crée une liste `primes` de valeurs booléennes, initialement supposant que tous les nombres sont premiers.\n",
            "    * `primes[0] = primes[1] = False`: Définit 0 et 1 comme non-premiers.\n",
            "* **Itération:**\n",
            "    * `for i in range(2, int(n**0.5) + 1):`: Itère de 2 jusqu'à la racine carrée de `n`. Nous ne devons vérifier que jusqu'à la racine carrée car tout nombre composite doit avoir un facteur premier inférieur ou égal à sa racine carrée.\n",
            "    * `if primes[i]:`: Si `i` est considéré comme premier:\n",
            "        * `for j in range(i * i, n + 1, i):`:  Marquer tous les multiples de `i` comme non-premiers.\n",
            "* **Resultat:**\n",
            "    * `return [i for i, is_prime in enumerate(primes) if is_prime]`: Crée une liste des indices où `primes[i]` est vrai, représentant les nombres premiers. \n",
            "\n",
            "\n",
            "\n",
            "<end_of_turn>\n"
          ]
        }
      ],
      "source": [
        "print(chat.get_history())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatState(gemma_lm)\n",
        "message = \"Tell me, in a few words,  how can I build a paperboat?\"\n",
        "display_chat(message, chat.send_message(message))"
      ],
      "metadata": {
        "id": "pI1wpshzwVnN",
        "outputId": "98ad1c22-32b1-409d-c2be-ab0518cbce69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<font size='+1' color='brown'>🙋‍♂️<blockquote>Tell me, in a few words,  how can I build a paperboat?</blockquote></font><font size='+1' color='teal'>🤖\n\n> Fold a square sheet into a boat shape. ⛵ \n> <end_of_turn>\n</font>"
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = \"now, tell me step by step to fold the sheet?\"\n",
        "display_chat(message, chat.send_message(message))"
      ],
      "metadata": {
        "id": "NsSnDQzxwmyK",
        "outputId": "b7fe1c45-347c-4c9b-dded-c0ced94c5713",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<font size='+1' color='brown'>🙋‍♂️<blockquote>now, tell me step by step to fold the sheet?</blockquote></font><font size='+1' color='teal'>🤖\n\n> Here's how to fold a simple paper boat:\n> \n> 1. **Start with a square sheet of paper.**\n> 2. **Fold the paper in half diagonally, crease well.**\n> 3. **Open the paper back up.**\n> 4. **Fold the top corner down to the center crease.**\n> 5. **Fold the bottom corner up to meet the top corner.**\n> 6. **Fold the paper in half again, along the original center crease.**\n> 7. **Fold the top edge down to the center crease.**\n> 8. **Fold the bottom edge up to meet the top edge.**\n> 9. **You should now have a small boat shape!**\n> \n> **Optional:** You can add a little extra detail by folding the sides in slightly to make the boat more sturdy. \n> \n> \n> Let me know if you'd like to see a visual! \n> <end_of_turn>\n</font>"
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = \"yes, show me the visual\"\n",
        "display_chat(message, chat.send_message(message))"
      ],
      "metadata": {
        "id": "k3VjM7y_wzBf",
        "outputId": "a44c824c-cb79-47e0-c904-de48440e7dd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<font size='+1' color='brown'>🙋‍♂️<blockquote>yes, show me the visual</blockquote></font><font size='+1' color='teal'>🤖\n\n> I can't *show* you a visual directly, as I'm a text-based AI.  \n> \n> However, I can help you find a visual!  Try searching for \"how to make a paper boat\" on Google Images or YouTube. You'll find lots of step-by-step videos and pictures to guide you. \n> \n> Let me know if you have any other questions! ⛵ \n> <end_of_turn>\n</font>"
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat.get_history())"
      ],
      "metadata": {
        "id": "UOQFBakSw92-",
        "outputId": "1f55d7df-59e0-4000-a360-668d2dd10967",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start_of_turn>user\n",
            "Tell me, in a few words,  how can I build a paperboat?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Fold a square sheet into a boat shape. ⛵ \n",
            "<end_of_turn><start_of_turn>user\n",
            "now, tell me step by step to fold the sheet?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Here's how to fold a simple paper boat:\n",
            "\n",
            "1. **Start with a square sheet of paper.**\n",
            "2. **Fold the paper in half diagonally, crease well.**\n",
            "3. **Open the paper back up.**\n",
            "4. **Fold the top corner down to the center crease.**\n",
            "5. **Fold the bottom corner up to meet the top corner.**\n",
            "6. **Fold the paper in half again, along the original center crease.**\n",
            "7. **Fold the top edge down to the center crease.**\n",
            "8. **Fold the bottom edge up to meet the top edge.**\n",
            "9. **You should now have a small boat shape!**\n",
            "\n",
            "**Optional:** You can add a little extra detail by folding the sides in slightly to make the boat more sturdy. \n",
            "\n",
            "\n",
            "Let me know if you'd like to see a visual! \n",
            "<end_of_turn><start_of_turn>user\n",
            "yes, show me the visual<end_of_turn>\n",
            "<start_of_turn>model\n",
            "I can't *show* you a visual directly, as I'm a text-based AI.  \n",
            "\n",
            "However, I can help you find a visual!  Try searching for \"how to make a paper boat\" on Google Images or YouTube. You'll find lots of step-by-step videos and pictures to guide you. \n",
            "\n",
            "Let me know if you have any other questions! ⛵ \n",
            "<end_of_turn>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9693c66f"
      },
      "source": [
        "## Summary and further reading\n",
        "\n",
        "In this tutorial, you learned how to chat with the Gemma 2B Instruction tuned model using Keras on JAX.\n",
        "\n",
        "Check out these guides and tutorials to learn more about Gemma:\n",
        "\n",
        "* [Get started with Keras Gemma](https://ai.google.dev/gemma/docs/get_started).\n",
        "* [Finetune the Gemma model on GPU](https://ai.google.dev/gemma/docs/lora_tuning).\n",
        "* Learn about [Gemma integration with Vertex AI](https://ai.google.dev/gemma/docs/integrations/vertex)\n",
        "* Learn how to [use Gemma models with Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/open-models/use-gemma){:.external}.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "gemma_chat.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}